{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "import jieba.analyse as aly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1)使用jieba利用TF-IDF的extract_tags()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言 2.08698834984\n",
      "人机交互 0.57078855011\n",
      "涉及 0.44150044929000004\n",
      "计算机 0.340239221544\n",
      "挑战 0.32964024356050003\n",
      "理解 0.3021791893535\n",
      "KLP 0.2988691875725\n",
      "NLP 0.2988691875725\n",
      "领域 0.27061477712049997\n",
      "计算机科学 0.2439385731395\n"
     ]
    }
   ],
   "source": [
    "content='''自然语言处理〈KLP）是计算机科学，人工智能，语言学关注计算机和人类〈自然）语言之间的相互作用的领域。\n",
    "因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。\n",
    "在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。\n",
    "'''\n",
    "\n",
    "#加载自定义idf词典\n",
    "#.set_idf_path./data/idf. txt.big')\n",
    "#加载停用词典\n",
    "aly.set_stop_words ('../Files/NLPIR_stopwords.txt')\n",
    "#第一个参数:待提取关键词的文本\n",
    "#第二个参数:TW/IDF权重最大返回关键词的数虽，重要性从高到低排序，默认值为20\n",
    "#第三个参数:是否同时返回每个关键词的权重，默认值为False\n",
    "#第四个参数:词性过滤，为空表示不过滤（默认），若提供则仅返回符合词性要求的关键词\n",
    "keywords = aly.extract_tags(content,topK=10,withWeight=True,allowPOS=())\n",
    "for item in keywords:\n",
    "#分别为关键词和相应的权重\n",
    "    print(item[0],item[1])\n",
    "#jieba. analyse.TFIDF(idf_path=None)#新建TFIDF实例，idf_path 为IDF频率文件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)使用textrank（）方法\n",
    "不需要数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算机 1.0\n",
      "涉及 0.9098683588859592\n",
      "理解 0.896989925199996\n",
      "挑战 0.776312239218466\n",
      "语言学 0.7383967053476187\n",
      "人机交互 0.6388072272830191\n",
      "人类 0.637171234555651\n",
      "源于 0.6108093584442846\n",
      "人工智能 0.5892237848656273\n",
      "关注 0.586742120184303\n"
     ]
    }
   ],
   "source": [
    "content='''自然语言处理〈KLP）是计算机科学，人工智能，语言学关注计算机和人类〈自然）语言之间的相互作用的领域。\n",
    "因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。\n",
    "在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。\n",
    "'''\n",
    "\n",
    "#第一个参教:待提取关键词的文本\n",
    "#第二个参数:返回关键词的数量,重要性从高到低排序\n",
    "#第三个参数:是否同时返回每个关键词的权重\n",
    "#第四个参数:词性过滤，为空表示过滤所有，与TF一IDF不一样\n",
    "#allowPoS('ns', 'n', 'vn', 'v）地名、名词、动名词、动词\n",
    "keywords = jieba.analyse. textrank(content,topK=10,withWeight=True, allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "#keywords = jieba. analyse. textrank (content,topK=10, withWeight=True，allowPOS=())\n",
    "for item in keywords :\n",
    "    #分别为关键词和相应的权重\n",
    "    print(item[0],item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)两种方法的比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧亚 0.8169207307466667\n",
      "吉林 0.737495111084762\n",
      "置业 0.5468936250935714\n",
      "万元 0.3796618015414286\n",
      "增资 0.3758030698347619\n",
      "4.3 0.28463732149761906\n",
      "7000 0.28463732149761906\n",
      "2013 0.28463732149761906\n",
      "139.13 0.28463732149761906\n",
      "综合体 0.2179939410383333\n",
      "----------------------------------------\n",
      "TextRank\n",
      "----------------------------------------\n",
      "吉林 1.0\n",
      "欧亚 0.9966569988559338\n",
      "置业 0.6434011646740962\n",
      "增资 0.40999103803840964\n",
      "营业 0.37744180744957695\n",
      "收入 0.3747986193976168\n",
      "子公司 0.3567665938504673\n",
      "城市 0.3496956136360638\n",
      "商业 0.3481603803364026\n",
      "业务 0.3092037492279567\n"
     ]
    }
   ],
   "source": [
    "s ='''\n",
    "此外，公司拟对全资子公司吉林欧亚置业有限公司增资4.3亿元，增资后，吉林欧亚置业注册资本由7000万元增加到5亿元。吉林欧亚置业主要经营范围为房地产开发及百货零售等业务。目前在建吉林欧亚城市商业综合体项目。\n",
    "2013年,实现营业收入0万元,实现净利润-139.13万元。\n",
    "'''\n",
    "for x, w in jieba.analyse.extract_tags(s,10,withWeight=True):\n",
    "    print ('%s %s'%(x,w))\n",
    "    \n",
    "print('-'*40)\n",
    "print('TextRank')\n",
    "print('-'*40)\n",
    "\n",
    "#textrank (sentence， topt=20, witheight=False, allowPOS=('ns', 'n' , 'vn'， 'v')）直接使用，接口相同，注意默认过德词性。\n",
    "for x, w in jieba.analyse.textrank(s,10, withWeight=True):\n",
    "    print('%s %s' % (x, w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、主题模型及综合用例\n",
    "下面用三种方法分别提取文本中的关键词:自己编写的TF-DF方法，Jieba中的TextRank方法，Gensim的主题模型法(LSI、 LDA)。  \n",
    "\n",
    "三种方法各有特点:\n",
    "TF-IDF方法根据数据集生成对应的IDF值字典，计算每个词的IDF时，直接从字典读取;  \n",
    "LS和LDA的训练是根据现有的数据集生成文档-主题分布矩阵和主题-词分布矩阵，可直接调用Gensim中方法  \n",
    "\n",
    "训练—个关键词提取算法需要以下几个步骤:  \n",
    ".1)加载已有的文本数据集  \n",
    ".2)加载停用词表  \n",
    ".3)对数据集中的文档进行分词  \n",
    ".4)根据停用词表，过滤干扰词  \n",
    ".5)根据数据集训练算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-9f8056b5153d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mposseg\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjieba\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0manalyse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import jieba\n",
    "import jieba. posseg as psg\n",
    "from gensim import corpora,models\n",
    "from jieba import analyse\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、TF-IDF计算\n",
    "(1)定义通用的预处理方法  \n",
    "包括:加载数据、停用词加载、分词、去除干扰词、文本的IDF统计、排序等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用词表加载方法\n",
    "#数据加载，pos为是否词性标注的参数，corpus_path为数据集路径\n",
    "def load_data(pos=False,corpus_path='../../文档/复旦新闻语料/CSCMNews/科技/481681.txt'):\n",
    "#调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    doc_list =[]\n",
    "    for line in open(corpus_path, 'r' , encoding='utf-8'):\n",
    "        content = line. strip()\n",
    "        seg_list =seg_to_list(content,pos)#分词\n",
    "        filter_list = word_filter(seg_list,pos)#去除干扰词\n",
    "        doc_list. append(filter_list)\n",
    "\n",
    "    return doc_list\n",
    "def get_stopword_list():\n",
    "    #停用词表存储路径,每一行为一个词,按行读取进行加载\n",
    "    #进行编码转换确保匹配准确季\n",
    "    stop_word_path = '../Files/NLPIR_stopwords.txt'\n",
    "    stopword_list= [sw.replace('\\n', '') for sw in open(stop_word_path, encoding='utf-8').readlines()]\n",
    "    return stopword_list\n",
    "\n",
    "#分词方法,调用结巴接口\n",
    "def seg_to_list(sentence,pos=False):\n",
    "    if not pos:\n",
    "    #不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut (sentence)\n",
    "    else:\n",
    "    #进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "#去除干扰词\n",
    "#输入参数: seg_list为待过滤文本序列;pos表示是否进行词性过滤\n",
    "def word_filter(seg_list,pos=False):\n",
    "    stopword_list = get_stopword_list()\n",
    "    filter_list =[]\n",
    "    #根痞PoS参教选择是否词性过滤\n",
    "    ##不进行词性过辣,则将词性都标记为n，丧示全部保留\n",
    "    for seg in seg_list :\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag='n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        #过滤停用词表中的词\n",
    "        if not word in stopword_list and len(word) >1:\n",
    "            filter_list. append(word)\n",
    "    return filter_list\n",
    "\n",
    "\n",
    "#idf值统计方法\n",
    "#注意:为何IDF统计方法作为通用的方法?因为是对整个语料集进行统计!\n",
    "def train_idf (doc_list):\n",
    "    idf_dic={}\n",
    "    #总文档数\n",
    "    tt_count = len(doc_list)\n",
    "    #每个词出现的文档数\n",
    "    for doc in doc_list :\n",
    "        for word in set(doc): #set()函数创建一个无序不重复元素集\n",
    "            idf_dic[word] = idf_dic.get(word,0.0) +1.0\n",
    "\n",
    "    #按公式转换为idf信,分母加1进行平滑处理\n",
    "    for k, v in idf_dic.items():\n",
    "        idf_dic[k] =math.log(tt_count/ (1.0 +v))\n",
    "    #对于没有在字典中的词，默认其仅在一个文档出现，得到默认idf值\n",
    "    default_idf= math.log(tt_count/ (1.0))\n",
    "    return idf_dic,default_idf\n",
    "\n",
    "#排序函数,用于topK关键词的按值排序\n",
    "def cmp(e1,e2):\n",
    "    import numpy as np\n",
    "    res = np.sign(e1[1] - e2[1])\n",
    "    if res !=0:\n",
    "        return res\n",
    "    else:\n",
    "        a = e1[0] +e2[0]#这里a和b不同,因为是字符串相加\n",
    "        b= e2[0]+e1[0]\n",
    "        if a > b:\n",
    "            return 1\n",
    "        elif a== b:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 定义TF-IDF类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF类\n",
    "class TfIdf(object):\n",
    "#四个参数分别是:训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量\n",
    "    def __init__(self, idf_dic, default_idf, word_list,keyword_num):\n",
    "        self.word_list = word_list\n",
    "        self.idf_dic, self.default_idf = idf_dic,default_idf\n",
    "        self.tf_dic = self.get_tf_dic()\n",
    "        self.keyword_num= keyword_num\n",
    "\n",
    "        #统计tf值\n",
    "    def get_tf_dic(self):\n",
    "        tf_dic = {}\n",
    "        for word in self.word_list :\n",
    "            tf_dic[word] = tf_dic. get(word,0.0)+ 1.0\n",
    "        \n",
    "        tt_count = len(self.word_list)\n",
    "        for k, v in tf_dic.items():\n",
    "            tf_dic[k] = float(v) / tt_count\n",
    "        return tf_dic\n",
    "\n",
    "        #按公式计算tf-idf\n",
    "    def get_tfidf(self):\n",
    "        tfidf_dic={}\n",
    "        for word in self.word_list :\n",
    "            idf = self.idf_dic.get(word,self.default_idf)#如果指定键的值不存在时，返回默认信self. default_idf。\n",
    "            tf = self.tf_dic.get (word,0)\n",
    "            tfidf = tf * idf\n",
    "            tfidf_dic[word] = tfidf\n",
    "        tfidf_dic.items()#以列表返回可遍历的(键，值》元组数组。\n",
    "            #根据tf-idf排序，去排名前keyword_rum的词作为关键词\n",
    "        for k, v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp),reverse=True)[:self.keyword_num]:\n",
    "                #functoois. cmp_to_key(cmp)是将比较函数( comparison function）转化为关键字函数(key function)\n",
    "            print(k + \"/ \", end='')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 定义主题模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主题模型\n",
    "class TopicModel(object):\n",
    "    #三个传入参数:处理后的数据集，关键词数量，具体模型（LSI、LDA)，主题数量\n",
    "    def __init__(self,doc_list,keyword_num,model='LSI',num_topics=4):\n",
    "        #使用gensim的接口,将文本转为向量化表示\n",
    "        #先构建词空间\n",
    "        #corpora.Dictionary对象可以理解为python中的字典对象，其Key是字典中的词，其Val是词对应的唯一数值型ID构造方法\n",
    "        self.dictionary = corpora. Dictionary(doc_list)\n",
    "        #使用BOW模型向量化\n",
    "    #由doc2bow变为词袋，输出的格式为:[[O, 1)，(，1)，(2， 1)，(3，2)，(4,，1)]例如(3，2)这个元素代表文档中i d为3的单词出现\n",
    "        corpus = [self.dictionary.doc2bow (doc) for doc in doc_list]\n",
    "        #print ('getcorpus',corpus)\n",
    "    #corpus是一个返回bow向量的迭代器。下面代码将完成对corpus中出现的每一个特征的IDP值的统计工作\n",
    "    #对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "        self.tfidf_model = models.TfidfModel(corpus)#定义TIDF模型\n",
    "        self.corpus_tfidf = self.tfidf_model[corpus]#用语料训练模型并生成TFIDF矩阵\n",
    "\n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "        #选择加载的模型\n",
    "        if model == 'LSI':\n",
    "            self.model = self.train_lsi()\n",
    "        else:\n",
    "            self.model = self.train_lda()\n",
    "        #得到数据集的主题-词分布\n",
    "        word_dic = self.word_dictionary(doc_list)\n",
    "        self.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    "    \n",
    "    def train_lsi(self):\n",
    "        lsi = models.LsiModel(self.corpus_tfidf,id2word=self.dictionary,num_topics=self.num_topics)\n",
    "        return lsi\n",
    "    def train_lda(self):\n",
    "        lda = models. LdaModel(self.corpus_tfidf,id2word=self.dictionary,num_topics=self.num_topics)\n",
    "        return lda\n",
    "\n",
    "    #词空间构建方法和向邑化方法,在没有gensim接口时的一般处理方法\n",
    "    def word_dictionary(self,doc_list):\n",
    "        dictionary = []\n",
    "        for doc in doc_list :\n",
    "                dictionary.extend(doc)\n",
    "        dictionary = list(set(dictionary))\n",
    "        return dictionary\n",
    "\n",
    "    def doc2bowvec(self, word_list):\n",
    "        vec_list = [1 if vord in word_list else 0 for word in self. dictionary]\n",
    "        return vec_list\n",
    "\n",
    "    def get_wordtopic(self, word_dic):\n",
    "        wordtopic_dic={}\n",
    "        ################################################################\n",
    "        for word in word_dic:\n",
    "            single_list =[word]\n",
    "            #print('words=',word)\n",
    "            wordcorpus = self.tfidf_model[self.dictionary. doc2bow(single_list)]\n",
    "            #print ('wordcorpus= ',wordcorpus)\n",
    "            wordtopic = self.model [wordcorpus]#根据相应的模型进行训练\n",
    "            #print('wordtopic=',wordtopic)\n",
    "            wordtopic_dic[word] = wordtopic\n",
    "        return wordtopic_dic\n",
    "\n",
    "    #计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词\n",
    "    def get_simword(self, word_list):\n",
    "        sentcorpus = self.tfidf_model[self.dictionary. doc2bow(word_list)]\n",
    "        senttopic = self.model [sentcorpus]\n",
    "        #################################################\n",
    "        #print ('word_list=',word_list)\n",
    "        #print('doc2bow= ', self. dictionary. doc2bow(word_list))\n",
    "        #print('sentcorpus=' , sentcorpus)\n",
    "        #print('senttopic= ',senttopic)\n",
    "\n",
    "        #佘弦相似度计算\n",
    "        def calsim(l1,l2):\n",
    "            a,b,c = 0.0,0.0,0.0\n",
    "            for t1,t2 in zip(l1,l2):\n",
    "                x1 = t1[1]\n",
    "                x2= t2[1]\n",
    "                a += x1*x2\n",
    "                b += x1*x1\n",
    "                c += x2*x2\n",
    "            sim = a / math.sqrt(b* c) if not (b* c)== 0.0 else 0.0\n",
    "            return sim\n",
    "        \n",
    "        #计算输入文本和每个词的主题分布相似度\n",
    "        sim_dic ={}\n",
    "        for k,v in self.wordtopic_dic.items():\n",
    "            if k not in word_list :\n",
    "                continue\n",
    "            sim = calsim(v, senttopic)\n",
    "            sim_dic[k] = sim\n",
    "        for k,v in sorted(sim_dic.items(),key=functools.cmp_to_key(cmp), reverse=True)[:self. keyword_num]:\n",
    "            print (k +\"/\", end='')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4)封装算法，统一算法调用接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用按照理论自己编写的T-IDF方法\n",
    "def tfidf_extract(word_list,pos=False,keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    idf_dic, default_idf = train_idf(doc_list)#根掘语料集统计idf\n",
    "    tfidf_model = TfIdf(idf_dic, default_idf,word_list, keyword_num)\n",
    "    tfidf_model.get_tfidf()#按照tf-idf排序\n",
    "    \n",
    "#调用jieba中的textrark方法\n",
    "def textrank_extract(text,pos=False, keyword_num=10):\n",
    "    textrank = analyse.textrank\n",
    "    keywords = textrank (text,keyword_num)\n",
    "    #输出抽取出的关键词\n",
    "    for keyword in keywords:\n",
    "        print (keyword +\"/\", end='')\n",
    "    print ()\n",
    "\n",
    "def topic_extract(word_list,model,pos=False,keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    topic_model = TopicModel(doc_list, keyword_num,model=model)\n",
    "    topic_model.get_simword(word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ( 5)应用实例\n",
    "根据训练好的关键词提取算法对新文档进行关键词提取需要经过以下环节:  \n",
    "1)对新文档诞行分词  \n",
    "·2)根据停用词表，过滤干扰词  \n",
    "·3)根据训练好的算法提取关键词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\A\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.923 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF模型结果:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'functools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9c8197459f80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mfilter_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'TF-IDF模型结果:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtfidf_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' \\nTextRank模型结果:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mtextrank_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-3121a221805b>\u001b[0m in \u001b[0;36mtfidf_extract\u001b[1;34m(word_list, pos, keyword_num)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0midf_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#根掘语料集统计idf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtfidf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfIdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midf_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_idf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtfidf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#按照tf-idf排序\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#调用jieba中的textrark方法\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-f2e7e14990b4>\u001b[0m in \u001b[0;36mget_tfidf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mtfidf_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#以列表返回可遍历的(键，值》元组数组。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m#根据tf-idf排序，去排名前keyword_rum的词作为关键词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmp_to_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyword_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                 \u001b[1;31m#functoois. cmp_to_key(cmp)是将比较函数( comparison function）转化为关键字函数(key function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/ \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'functools' is not defined"
     ]
    }
   ],
   "source": [
    "text ='6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。'+ \\\n",
    "'中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄'+ \\\n",
    "'办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,'+ \\\n",
    "'重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局'+ \\\n",
    "'领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城'+ \\\n",
    "'市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市'+ \\\n",
    "'、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加'+\\\n",
    "'这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动'+ \\\n",
    "'的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理'+ \\\n",
    "'事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值4o0万元的款物。晋江市人大'+ \\\n",
    "'常委会主任陈健倩介绍了大会的筹备情况。'\n",
    "\n",
    "pos =True\n",
    "seg_list = seg_to_list(text,pos)\n",
    "filter_list = word_filter(seg_list,pos)\n",
    "print ('TF-IDF模型结果:')\n",
    "tfidf_extract(filter_list)\n",
    "print(' \\nTextRank模型结果:')\n",
    "textrank_extract(text)\n",
    "print ( '\\nLSI模型结果:')\n",
    "topic_extract (filter_list,'LSI', pos)\n",
    "print ('\\nLDA模型结果:')\n",
    "topic_extract(filter_list,'LDA', pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
